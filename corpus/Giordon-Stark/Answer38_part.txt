 Warning: severe usage of acronyms.This is a tricky question. There is so much history and experience (> 20 years, countless million hours of man power) behind data storage. First, when it comes to grid computing at CERN and LHC, all resources are collected into a unifying location[1].Next, there are a lot of experiments at CERN:ATLAS (I work with this)ALICECMSLHCband each experiment does something different. Because of the large amount of information and not trying to explain them all, I'll only talk about ATLAS (A Large Toroidal LHC Apparatus).In Run 1, from 2008-2013 - ATLAS had a computing model that was at the top level Tier, frozen. This meant, in large part, the data format was decided ahead of time and "frozen" in place, so even if changes were going to be made, it would only happen after commissioning and Phase 0 upgrades[2]. This kind of output data from the detector is not readable in ROOT, so people spent a large amount of time translating the data into something that was ROOT-readable.which is now superseded with the new Analysis model that allows for re-derived, re-skimmed, re-slimmed, re-thinned, re-reduced datasets in nearly real-time -- even as the datasets are on the order of petabytes and terabytes!And there is an AOD2AOD in the bottom left of the figure which means that datasets we've reconstructed from the top-level tier can be re-run with the update calibrations, systematics, and improved reconstruction techniques on a periodic basis during the operator of the LHC detector.So what kind of data is actually involved/stored in the first place?which is a significant amount. Most Physicists in Run-1 dealt with NTUPLs and DPDs -- these are your standard ROOT files with TTrees and TBranches and TLeaves (I'll get to these later). Now, when we gear up for Run-2 we will work on DxAOD primarily which are derived (gigabytes in size) from AODs (terabytes in size) formed from the Tier-0 AODs (petabytes in size). Here's a picture highlighting the general processWhat is Tier-0 then? Computing sites?Well, Tier-0 refers to the facility at CERN that had the primary role of interacting directly with the RAW data from the detector as well as the archival of the data on magnetic tapes.The LHC grid is organized into tiers. Tier 0 is at CERN itself and consists in large part of thousands of commercially bought servers, both PC-style boxes and, more recently, blade systems looking like black pizza boxes, stacked in row after row of shelves. Computers are still being purchased and added to the system. The data passed to Tier 0 by the LHC data-acquisition systems will be archived on magnetic tape.Tier 0 will distribute the data to the 12 Tier 1 centers, which are located at CERN itself and at 11 other major institutes around the world. Thus, the unprocessed data will exist in two copies, one at CERN and one divided up around the world. Each of the Tier 1 centers will also host a complete set of the data in a compact form structured for physicists to carry out many of their analyses.The full LHC Computing Grid also has Tier 2 centers, which are smaller computing centers at universities and research institutes. Computers at these centers will supply distributed processing power to the entire grid for the data analyses.[4]The Worldwide LHC Computing Grid (WLCG) is composed of four levels, or Tiers, called 0, 1, 2 and 3. Each Tier is made up of several computer centres and provides a specific set of services. Between them the tiers process, store and analyse all the data from the Large Hadron Collider(LHC).Tier 0 is the CERN Data Centre. All of the data from the LHC passes through this central hub, but it provides less than 20% of the Grid's total computing capacity. CERN is responsible for the safe keeping of the raw data (millions of digital readings from across the detectors), and performs the first pass at reconstructing the raw data into meaningful information. Tier 0 distributes the raw data and the reconstructed output to Tier 1s, and reprocesses data when the LHC is not running.Tier 1 consists of 13 computer centres (table 1) large enough to store LHC data. They provide round-the-clock support for the Grid, and are responsible for storing a proportional share of raw and reconstructed data, as well as performing large-scale reprocessing and storing the corresponding output; distributing data to Tier 2s; and storing a share of the simulated data that the Tier 2s produce. Optical-fibre links working at 10 gigabits per second connect CERN to each of the 13 major Tier 1 centres around the world. This dedicated high-bandwidth network is called the LHC Optical Private Network (LHCOPN).Why is the data size so big?When the LHC is operational for Run 2, from early 2015 through to 2018, it will be processing events at a rate of 40 MHz (Megahertz). In other words, 40 million events per second. This is a staggering 25 nanoseconds between collisions at an unprecedented energy level. The rule of thumb on data size per event is [math] 1 \text{ event} \approx 1 \text{ MB}[/math] [7]which is 40 terabytes per second of data coming in. PER. SECOND. This is a lot of information, and we would fill up a single petabyte in about half a minute. It is impossible for us to record all that information. So we need to filter events at the same rate at which they come in. This is where the levels come in. I made the following cartoon to highlight the overall system summarized from the Level-1 Technical Data Report (L1TDR)[6].To make this clear, imagine that water flows through the system from top to bottom. The red circles (Central Trigger Processor, CTP) can open up the pipes to the next level (from level 1 to level 2, and from level 2 to the magnetic tapes at Tier-0 for recording). The goal is to trigger on events that we consider interesting, but trigger fast enough that we don't have a backlog. Level 1 is basically our first line of defense. One of my research projects is in hardware development for this level for Run 3 (2018+). So Level 1 is incredibly crude and incredibly fast. It has multiple components that analyze various parts of the event in parallel (via data multiplexing). There are lots of buffers, pipelines, Read-Out Disks, etc... all helping us move all this information through the pipelineThis is a schematic overview of what is attached to the first red dot for the Level 1 trigger. The CTP on the bottom is that