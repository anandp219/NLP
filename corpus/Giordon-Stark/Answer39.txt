 Warning: severe usage of acronyms.This is a tricky question. There is so much history and experience (> 20 years, countless million hours of man power) behind data storage. First, when it comes to grid computing at CERN and LHC, all resources are collected into a unifying location[1].Next, there are a lot of experiments at CERN:ATLAS (I work with this)ALICECMSLHCband each experiment does something different. Because of the large amount of information and not trying to explain them all, I'll only talk about ATLAS (A Large Toroidal LHC Apparatus).In Run 1, from 2008-2013 - ATLAS had a computing model that was at the top level Tier, frozen. This meant, in large part, the data format was decided ahead of time and "frozen" in place, so even if changes were going to be made, it would only happen after commissioning and Phase 0 upgrades[2]. This kind of output data from the detector is not readable in ROOT, so people spent a large amount of time translating the data into something that was ROOT-readable.which is now superseded with the new Analysis model that allows for re-derived, re-skimmed, re-slimmed, re-thinned, re-reduced datasets in nearly real-time -- even as the datasets are on the order of petabytes and terabytes!And there is an AOD2AOD in the bottom left of the figure which means that datasets we've reconstructed from the top-level tier can be re-run with the update calibrations, systematics, and improved reconstruction techniques on a periodic basis during the operator of the LHC detector.So what kind of data is actually involved/stored in the first place?which is a significant amount. Most Physicists in Run-1 dealt with NTUPLs and DPDs -- these are your standard ROOT files with TTrees and TBranches and TLeaves (I'll get to these later). Now, when we gear up for Run-2 we will work on DxAOD primarily which are derived (gigabytes in size) from AODs (terabytes in size) formed from the Tier-0 AODs (petabytes in size). Here's a picture highlighting the general processWhat is Tier-0 then? Computing sites?Well, Tier-0 refers to the facility at CERN that had the primary role of interacting directly with the RAW data from the detector as well as the archival of the data on magnetic tapes.The LHC grid is organized into tiers. Tier 0 is at CERN itself and consists in large part of thousands of commercially bought servers, both PC-style boxes and, more recently, blade systems looking like black pizza boxes, stacked in row after row of shelves. Computers are still being purchased and added to the system. The data passed to Tier 0 by the LHC data-acquisition systems will be archived on magnetic tape.Tier 0 will distribute the data to the 12 Tier 1 centers, which are located at CERN itself and at 11 other major institutes around the world. Thus, the unprocessed data will exist in two copies, one at CERN and one divided up around the world. Each of the Tier 1 centers will also host a complete set of the data in a compact form structured for physicists to carry out many of their analyses.The full LHC Computing Grid also has Tier 2 centers, which are smaller computing centers at universities and research institutes. Computers at these centers will supply distributed processing power to the entire grid for the data analyses.[4]The Worldwide LHC Computing Grid (WLCG) is composed of four levels, or Tiers, called 0, 1, 2 and 3. Each Tier is made up of several computer centres and provides a specific set of services. Between them the tiers process, store and analyse all the data from the Large Hadron Collider(LHC).Tier 0 is the CERN Data Centre. All of the data from the LHC passes through this central hub, but it provides less than 20% of the Grid's total computing capacity. CERN is responsible for the safe keeping of the raw data (millions of digital readings from across the detectors), and performs the first pass at reconstructing the raw data into meaningful information. Tier 0 distributes the raw data and the reconstructed output to Tier 1s, and reprocesses data when the LHC is not running.Tier 1 consists of 13 computer centres (table 1) large enough to store LHC data. They provide round-the-clock support for the Grid, and are responsible for storing a proportional share of raw and reconstructed data, as well as performing large-scale reprocessing and storing the corresponding output; distributing data to Tier 2s; and storing a share of the simulated data that the Tier 2s produce. Optical-fibre links working at 10 gigabits per second connect CERN to each of the 13 major Tier 1 centres around the world. This dedicated high-bandwidth network is called the LHC Optical Private Network (LHCOPN).Why is the data size so big?When the LHC is operational for Run 2, from early 2015 through to 2018, it will be processing events at a rate of 40 MHz (Megahertz). In other words, 40 million events per second. This is a staggering 25 nanoseconds between collisions at an unprecedented energy level. The rule of thumb on data size per event is [math] 1 \text{ event} \approx 1 \text{ MB}[/math] [7]which is 40 terabytes per second of data coming in. PER. SECOND. This is a lot of information, and we would fill up a single petabyte in about half a minute. It is impossible for us to record all that information. So we need to filter events at the same rate at which they come in. This is where the levels come in. I made the following cartoon to highlight the overall system summarized from the Level-1 Technical Data Report (L1TDR)[6].To make this clear, imagine that water flows through the system from top to bottom. The red circles (Central Trigger Processor, CTP) can open up the pipes to the next level (from level 1 to level 2, and from level 2 to the magnetic tapes at Tier-0 for recording). The goal is to trigger on events that we consider interesting, but trigger fast enough that we don't have a backlog. Level 1 is basically our first line of defense. One of my research projects is in hardware development for this level for Run 3 (2018+). So Level 1 is incredibly crude and incredibly fast. It has multiple components that analyze various parts of the event in parallel (via data multiplexing). There are lots of buffers, pipelines, Read-Out Disks, etc... all helping us move all this information through the pipelineThis is a schematic overview of what is attached to the first red dot for the Level 1 trigger. The CTP on the bottom is that red dot on the previous image I've shown you. The Level 1 calorimeter gets data from the Liquid-Argon and Tile Calorimeters of the detector, and does some specific processing such as calculating the amount of energy in various regions using a sliding-window algorithm, or doing a very crude clustering algorithm to identify regions of more energy density. The job of the CTP is to take the information from the Calorimeter, from the Muons, and combine it into a Global Level-1 Accept (L1A) that determines whether the event it currently looks at goes to Level-2 or if we just throw it away because it's not interesting enough.The end goal is to filter 40 million events per second to 1000 events per second, which can be recorded on magnetic tape.So what are ROOT files? [8]Well, ROOT files are... more or less... binary databases. They contain information about itself (description) as well as the raw data (data). Everything inside a ROOT file is binary for various computing reasons I'd rather not get into. Because of this, a program that understands how to read a ROOT file knows that all it needs to do is open it up and read the description to understand enough about it. This means that we can open and read ROOT files in O(1) time since it doesn't matter how large or small the file is, it only reads in some number of bytes to understand the structure of the file you've given it, and then contains pointers to more information if you request it. A ROOT file is like a database, and it contains lots of TDirectories and TTrees. A TTree is like a database table. A TDirectory is like a collection of database tables. A TTree can have TBranches/TLeaves which are like columns. And the rows are the events. Here is a code example to illustrate some basic structure: 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
kratsg@connect:~/faxbox/LArStudies$ root -b -l PileupSkim_Pythia8_AU2CT10_jetjet_JZ0W_0.root root [0] Attaching file PileupSkim_Pythia8_AU2CT10_jetjet_JZ0W_0.root as _file0...root [1] .lsTFile**		PileupSkim_Pythia8_AU2CT10_jetjet_JZ0W_0.root	 TFile*		PileupSkim_Pythia8_AU2CT10_jetjet_JZ0W_0.root	 KEY: TTree	mytree;1	mytreeroot [2] mytree->ls()OBJ: TTree	mytree	mytree : 0 at: 0x1dc2430root [3] mytree(class TTree*)0x1dc2430Notice how it tells me that the TTree mytree is located in memory at 0x1dc2430. ROOT understands how to find things because it's like a mailman given someone's postal address. Instead of giving the mailman my whole house which is pretty hard to carry, I just give him my address and he knows where to find my house when he needs it.Now, what's inside the TTree? Well, branches, lots of columns of data. We can get a sneakpeak at this in a variety of ways1
2
3
4
root [4] mytree->GetListOfBranches()(class TObjArray*)0x1dc2580root [5] mytree->GetListOfBranches()->GetEntries()(const Int_t)1050which tells me there are 1,050 columns in my database table. What's one of them?1
2
3
4
5
6
7
8
root [6] mytree->GetListOfBranches()->At(0)(const class TObject*)0x20ab610root [7] mytree->GetListOfBranches()->At(0)->GetName()(const char* 0x20ab629)"weight"root [8] mytree->GetListOfBranches()->At(129)->GetName()(const char* 0x241dea0)"Eventshape_rhoKt4EM"root [9] mytree->GetListOfBranches()->At(583)->GetName()(const char* 0x26eb9f0)"jet_AntiKt10TruthTrimmedPtFrac5SmallR30_ZCUT12"and so on. All of these are accessed almost instantaneously because all this information is simply descriptor. It has no idea what the actual data is because everything is retrieved via pointers. Because of this, we can slowly unfold and expand the data at various locations that we care about... and we know where to unpack the data because we know where everything is! We just don't know what it is. So let's see what the value is for some events, such as the ZCUT12 branch we've found.1
2
3
4
5
6
7
8
root [13] mytree->GetEntry(0)(Int_t)720490root [15] mytree->GetBranch("jet_AntiKt10TruthTrimmedPtFrac5SmallR30_ZCUT12")(class TBranch*)0x26eb520root [18] mytree->GetBranch("jet_AntiKt10TruthTrimmedPtFrac5SmallR30_ZCUT12")->GetEntry(0)(Int_t)10root [19] mytree->GetBranch("jet_AntiKt10LCTopo_pt")->GetEntry(0)(int_t) 12and so on. The return of the GetEntry function just tells you the size of the data stored at that particular entry. To get things like actual values, you need to grab the TLeaf associated with the TBranch (it's complicated, I know, but you generally get used to it or just write macros so you never deal with actual direct features like accessing a single value). One of the locations in which ROOT shines is that you can chain multiple ROOT files together and then draw histograms and save them to files. Drawing histograms is one of the most common things we can do and can be as easy as1
2
root [0] mytree->Draw("jet_AntiKt10LCTopo_pt")root [1] mytree->Draw("jet_AntiKt10LCTopo_pt:jet_AntiKt10LCTopo_m")the first draws a 1D histogram of the Anti-Kt R=1.0 (clustered from topological clusters in the detector) jet's pt. The second is a 2D histogram of the jet pt versus jet mass. The histogrammed data could be stored back in a ROOT file to access in another script to draw the plots and save them to file. Because of the nature of the ROOT file, we can access histograms as fast as we want independent of the size of the ROOT file because it stores pointers to the histogram data.Now, as you start getting significantly more complicated data, and the nature of the ROOT files able to store pointers to other data, you'll find that standard databases are ill-equipped to store ROOT data in the first place. For example, an Anti-Kt R=1.0 locally calibrated jet from topological clusters could be groomed with Kt, R=0.3 subjets. Rather than storing the subjet information directly under the jet, the jet stores pointers to the subjet information. This means that the same data could be pointed to by multiple sources in an effort to minimize storage space and maximize efficiency. I once tried to take a ROOT file that was 2 gigabytes and store it in a MySQL database (23 gigabytes) and a PostgreSQL database (18 gigabytes). ROOT does a fantastic job with compression and normally databases cannot handle this shit.[1] Welcome to the Worldwide LHC Computing Grid[2] http://cds.cern.ch/record/169540...[3] Large Hadron Collider[4] CERN LHC[5] The Grid: A system of tiers[6] http://atlas.web.cern.ch/Atlas/G...[7] Page on gantep.edu.tr[8] ROOT Files | ROOT[9] TTree[10] TBranch[11] TLeaf